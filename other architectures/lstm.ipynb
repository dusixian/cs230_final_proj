{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataloader import get_dataloaders, MAX_SEQ_LENGTH, vocab_size\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "\n",
    "class RNAPairLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1, device='cpu'):\n",
    "        super(RNAPairLSTM, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.device = device\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input): \n",
    "        h_0 = Variable(torch.zeros(2*self.num_layers, input.size(0), self.hidden_dim, requires_grad=False).to(self.device))\n",
    "        c_0 = Variable(torch.zeros(2*self.num_layers, input.size(0), self.hidden_dim).to(self.device))\n",
    "\n",
    "        output, (h_out, _) = self.lstm(input, (h_0, c_0))\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for seq1, seq2 in train_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            outputs = model(seq1)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), seq2.reshape(-1, seq2.size(-1)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "def evaluate_model(model, dev_loader, criterion, device):\n",
    "    def enforce_e_and_p(output):\n",
    "        E_token_index = 4\n",
    "        P_token_index = 5\n",
    "\n",
    "        output_indices = torch.argmax(output, dim=-1)\n",
    "        for i in range(output.size(0)):  # Iterate over batch\n",
    "            e_found = False\n",
    "            for j in range(output.size(1)):  # Iterate over sequence length\n",
    "                if e_found:\n",
    "                    output_indices[i, j] = P_token_index\n",
    "                elif output_indices[i, j] == E_token_index:\n",
    "                    e_found = True\n",
    "            if not e_found:\n",
    "                output_indices[i, -1] = E_token_index\n",
    "\n",
    "        # Convert indices back to one-hot encoding\n",
    "        output_one_hot = torch.zeros_like(output).scatter_(-1, output_indices.unsqueeze(-1), 1.0)\n",
    "        return output_one_hot\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for seq1, seq2 in dev_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            outputs = model(seq1)\n",
    "            outputs = enforce_e_and_p(outputs)  # Apply post-processing\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), seq2.reshape(-1, seq2.size(-1)))\n",
    "            total_loss += loss.item()\n",
    "        print(f'Dev Loss: {total_loss / len(dev_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n",
      "Epoch 1/30\n",
      "Epoch [1/30], Loss: 0.4937\n",
      "Epoch 2/30\n",
      "Epoch [2/30], Loss: 0.3978\n",
      "Epoch 3/30\n",
      "Epoch [3/30], Loss: 0.3760\n",
      "Epoch 4/30\n",
      "Epoch [4/30], Loss: 0.3440\n",
      "Epoch 5/30\n",
      "Epoch [5/30], Loss: 0.3256\n",
      "Epoch 6/30\n",
      "Epoch [6/30], Loss: 0.3022\n",
      "Epoch 7/30\n",
      "Epoch [7/30], Loss: 0.3165\n",
      "Epoch 8/30\n",
      "Epoch [8/30], Loss: 0.4658\n",
      "Epoch 9/30\n",
      "Epoch [9/30], Loss: 0.4865\n",
      "Epoch 10/30\n",
      "Epoch [10/30], Loss: 0.3924\n",
      "Epoch 11/30\n",
      "Epoch [11/30], Loss: 0.3884\n",
      "Epoch 12/30\n",
      "Epoch [12/30], Loss: 0.5550\n",
      "Epoch 13/30\n",
      "Epoch [13/30], Loss: 0.3202\n",
      "Epoch 14/30\n",
      "Epoch [14/30], Loss: 0.4354\n",
      "Epoch 15/30\n",
      "Epoch [15/30], Loss: 0.3606\n",
      "Epoch 16/30\n",
      "Epoch [16/30], Loss: 0.4685\n",
      "Epoch 17/30\n",
      "Epoch [17/30], Loss: 0.3050\n",
      "Epoch 18/30\n",
      "Epoch [18/30], Loss: 0.3041\n",
      "Epoch 19/30\n",
      "Epoch [19/30], Loss: 0.4928\n",
      "Epoch 20/30\n",
      "Epoch [20/30], Loss: 0.2942\n",
      "Epoch 21/30\n",
      "Epoch [21/30], Loss: 0.3686\n",
      "Epoch 22/30\n",
      "Epoch [22/30], Loss: 0.3971\n",
      "Epoch 23/30\n",
      "Epoch [23/30], Loss: 0.4006\n",
      "Epoch 24/30\n",
      "Epoch [24/30], Loss: 0.2873\n",
      "Epoch 25/30\n",
      "Epoch [25/30], Loss: 0.2532\n",
      "Epoch 26/30\n",
      "Epoch [26/30], Loss: 0.3241\n",
      "Epoch 27/30\n",
      "Epoch [27/30], Loss: 0.2902\n",
      "Epoch 28/30\n",
      "Epoch [28/30], Loss: 0.2609\n",
      "Epoch 29/30\n",
      "Epoch [29/30], Loss: 0.3408\n",
      "Epoch 30/30\n",
      "Epoch [30/30], Loss: 0.2263\n",
      "Dev Loss: 0.5330\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = vocab_size  # One-hot encoded input size\n",
    "hidden_dim = 128\n",
    "output_dim = vocab_size  # One-hot encoded output size\n",
    "num_layers = 2\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-2\n",
    "batch_size = 32\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print('Using ' + device)\n",
    "\n",
    "# Load data\n",
    "train_loader, dev_loader, test_loader = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "# Initialize model, criterion and optimizer\n",
    "model = RNAPairLSTM(input_dim, hidden_dim, output_dim, num_layers, device).to(device)\n",
    "weight = torch.tensor([1,1,1,1,1,0.01,1],dtype=torch.float32,requires_grad=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)  # Use CrossEntropyLoss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** epoch)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
    "# save model using dd/mm-hh:mm\n",
    "path = time.strftime(\"%d-%m-%H:%M\") + '.pth'\n",
    "# torch.save(model.state_dict(), 'model_test.pth')\n",
    "torch.save(model.state_dict(), 'model_test.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model, dev_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.5934857726097107\n",
      "seq1:  ['T', 'G', 'A', 'G', 'A', 'T', 'G', 'G', 'A', 'G', 'T', 'C', 'T', 'C', 'G', 'C', 'T', 'C', 'T', 'G', 'A', 'C', 'G', 'C', 'C', 'A', 'G', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'T', 'G', 'C', 'A', 'G', 'T', 'G', 'G', 'T', 'G', 'C', 'G', 'A', 'T', 'C', 'T', 'C']\n",
      "seq2:  ['G', 'A', 'G', 'T', 'C', 'T', 'T', 'G', 'C', 'T', 'C', 'T', 'G', 'T', 'C', 'G', 'C', 'C', 'T', 'A', 'G', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'T', 'G', 'C', 'A', 'G', 'T', 'G', 'G', 'C', 'G', 'C', 'G', 'A', 'T', 'C', 'T', 'C', 'G', 'G', 'C', 'T', 'C', 'A']\n",
      "pred:  ['G', 'A', 'G', 'T', 'C', 'T', 'T', 'G', 'C', 'T', 'T', 'T', 'G', 'T', 'C', 'T', 'C', 'G', 'T', 'G', 'C', 'T', 'C', 'T', 'G', 'G', 'G', 'G', 'T', 'G', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'A', 'T', 'A', 'T', 'C', 'G', 'G', 'C']\n",
      "train loss:  0.8530638217926025\n",
      "seq1:  ['G', 'G', 'G', 'T', 'T', 'T', 'T', 'C', 'C', 'T', 'C', 'G', 'G', 'G', 'C', 'C', 'G', 'C', 'C', 'C', 'T', 'C', 'C', 'G', 'G', 'G', 'T']\n",
      "seq2:  ['G', 'C', 'C', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'C', 'C', 'G', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "pred:  ['G', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "train loss:  0.1863543689250946\n",
      "seq1:  ['C', 'C', 'C', 'G', 'C', 'C', 'C']\n",
      "seq2:  ['G', 'G', 'G', 'C', 'G', 'T', 'G', 'G']\n",
      "pred:  ['G', 'G', 'G', 'T', 'G', 'G', 'C', 'G']\n",
      "train loss:  0.7431184649467468\n",
      "seq1:  ['T', 'G', 'C', 'A', 'A', 'C', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'C', 'T', 'G', 'G', 'G', 'C', 'T', 'C', 'A', 'A', 'G', 'C', 'G']\n",
      "seq2:  ['T', 'G', 'C', 'T', 'G', 'G', 'C', 'A', 'C', 'C', 'A', 'C', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'C', 'C', 'G', 'C', 'G', 'C', 'A', 'C', 'C', 'T', 'G', 'C', 'A']\n",
      "pred:  ['T', 'G', 'C', 'T', 'G', 'C', 'C', 'G', 'C', 'G', 'C', 'C', 'C', 'C', 'A', 'A', 'A', 'C', 'C', 'C', 'A', 'G', 'C', 'G', 'A', 'C', 'T', 'G']\n",
      "train loss:  0.02258017100393772\n",
      "seq1:  ['G', 'C', 'C', 'C', 'G', 'T']\n",
      "seq2:  ['G', 'C', 'T', 'C', 'G', 'C']\n",
      "pred:  ['G', 'C', 'T', 'C', 'G', 'C']\n",
      "train loss:  1.3394352197647095\n",
      "seq1:  ['T', 'A', 'G', 'G', 'A', 'T', 'G', 'G', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'A', 'G', 'A', 'C', 'A', 'A', 'G', 'G', 'C', 'C', 'T', 'T', 'G', 'C', 'T', 'G']\n",
      "seq2:  ['C', 'A', 'G', 'C', 'A', 'T', 'T', 'A', 'A', 'C', 'T', 'C', 'A', 'A', 'A', 'A', 'G', 'T', 'C', 'C', 'A', 'A', 'G', 'T', 'T', 'G', 'A', 'C', 'T', 'C', 'T', 'C', 'A', 'T', 'C', 'T', 'G', 'A', 'A', 'A', 'C', 'A', 'A', 'G', 'G', 'C', 'A', 'T', 'G', 'T', 'C', 'C', 'C', 'T', 'T', 'C', 'T', 'G', 'C', 'C', 'T', 'A']\n",
      "pred:  ['C', 'A', 'G', 'G', 'C', 'T', 'T', 'T', 'A', 'A', 'C', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'G', 'A', 'G', 'C', 'G', 'G', 'C', 'A', 'C', 'C', 'A', 'A', 'A', 'A', 'T', 'G', 'A', 'A', 'T', 'C', 'A', 'T', 'G', 'G', 'C', 'T', 'T', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "train loss:  0.9006585478782654\n",
      "seq1:  ['T', 'T', 'G', 'A', 'G', 'A', 'C', 'G', 'G', 'A', 'G', 'T', 'T', 'T', 'C', 'G', 'C', 'T', 'C', 'T', 'T', 'G', 'T', 'T', 'G', 'C', 'C', 'C', 'A', 'G', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'T', 'G', 'C', 'A', 'A', 'T', 'G', 'G', 'C', 'G']\n",
      "seq2:  ['C', 'G', 'C', 'T', 'C', 'A', 'C', 'T', 'G', 'C', 'A', 'A', 'C', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'C', 'T', 'G', 'G', 'G', 'T', 'T', 'C', 'A', 'A', 'G', 'C', 'G', 'A', 'T', 'T', 'C', 'T', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'C', 'A', 'G']\n",
      "pred:  ['C', 'G', 'C', 'T', 'C', 'A', 'G', 'T', 'G', 'T', 'C', 'C', 'C', 'T', 'T', 'C', 'C', 'C', 'C', 'A', 'T', 'A', 'T', 'C', 'C', 'T', 'C', 'T', 'T', 'C', 'T', 'T', 'G', 'T', 'C', 'T', 'T', 'G', 'C', 'T', 'T', 'C', 'T', 'G', 'C', 'C', 'T', 'G', 'C', 'C', 'A', 'C', 'C', 'C', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
      "train loss:  0.5360059142112732\n",
      "seq1:  ['A', 'C', 'T', 'T', 'T', 'G', 'T', 'G', 'T', 'A', 'C', 'A', 'T', 'T', 'T', 'C', 'T', 'G', 'T', 'T', 'G', 'T', 'G', 'T', 'C', 'T']\n",
      "seq2:  ['A', 'G', 'G', 'C', 'G', 'C', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'C', 'G', 'C', 'C', 'T', 'G', 'T']\n",
      "pred:  ['A', 'G', 'G', 'T', 'G', 'C', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'C', 'G', 'C', 'C', 'T', 'G', 'T']\n",
      "train loss:  3.258364200592041\n",
      "seq1:  ['G', 'G', 'G', 'G', 'C', 'T', 'C', 'A', 'T', 'G', 'C', 'C', 'T', 'G', 'T', 'A', 'A', 'T', 'C', 'C', 'C', 'A', 'G', 'C', 'A', 'C', 'T', 'T', 'T', 'G', 'G', 'G', 'A', 'G', 'G', 'C', 'C', 'G', 'A', 'G', 'G', 'C', 'G', 'G', 'G', 'C', 'G', 'G', 'A', 'T', 'C', 'A', 'C', 'C', 'T', 'G', 'A', 'G', 'G', 'T', 'C', 'G', 'G', 'G', 'A', 'G', 'T', 'T', 'C', 'G', 'A', 'G', 'A', 'C', 'C']\n",
      "seq2:  ['G', 'G', 'T', 'C', 'A', 'G', 'G', 'A', 'G', 'T', 'T', 'C', 'A', 'A', 'G', 'A', 'C', 'C', 'A', 'G', 'C', 'C', 'T', 'G', 'G', 'A', 'C', 'A', 'A', 'A', 'A', 'T', 'G', 'G', 'T', 'G', 'A', 'A', 'A', 'C', 'C', 'C', 'C']\n",
      "pred:  ['G', 'G', 'T', 'C', 'A', 'G', 'G', 'G', 'G', 'C', 'C', 'T', 'T', 'T', 'C', 'A', 'T', 'C', 'C', 'A', 'T', 'C', 'T', 'A', 'G', 'A', 'A', 'G', 'A', 'A', 'A', 'C', 'G', 'A', 'A', 'C', 'A', 'A', 'G', 'C', 'C', 'T', 'C', 'G', 'C', 'T', 'G', 'G', 'C', 'T', 'G', 'C', 'A', 'G', 'C', 'C', 'C', 'C', 'G', 'T', 'C', 'C', 'C', 'C', 'T', 'G', 'A', 'T', 'C', 'C', 'T', 'T', 'T', 'C', 'A']\n",
      "train loss:  0.4793171286582947\n",
      "seq1:  ['C', 'A', 'C', 'T', 'T', 'T', 'T', 'G', 'G', 'T']\n",
      "seq2:  ['G', 'C', 'T', 'T', 'T', 'A', 'G', 'T', 'G']\n",
      "pred:  ['G', 'T', 'T', 'G', 'G', 'G', 'G', 'G', 'T', 'G', 'T']\n",
      "dev samples\n",
      "train loss:  0.0019719121046364307\n",
      "seq1:  ['G', 'A', 'T', 'A']\n",
      "seq2:  ['T', 'G', 'T', 'C']\n",
      "pred:  ['T', 'G', 'T', 'C']\n",
      "train loss:  0.050396550446748734\n",
      "seq1:  ['G', 'A', 'A', 'A', 'G', 'A', 'G', 'A', 'C', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'G', 'T', 'C', 'C', 'C', 'T', 'G', 'G', 'T', 'C']\n",
      "pred:  ['C', 'T', 'G', 'T', 'C', 'T', 'C', 'T', 'G', 'G', 'T', 'C']\n",
      "train loss:  0.0009409354534000158\n",
      "seq1:  ['T', 'T', 'G']\n",
      "seq2:  ['C', 'A', 'A']\n",
      "pred:  ['C', 'A', 'A']\n",
      "train loss:  1.1564654111862183\n",
      "seq1:  ['T', 'G', 'A', 'T', 'C', 'A', 'T', 'G', 'C', 'C', 'G', 'C', 'T', 'G', 'C', 'A', 'T', 'T', 'C', 'C', 'A', 'G', 'C', 'C', 'T', 'G', 'G', 'G', 'T', 'G', 'A', 'C', 'A', 'G', 'A', 'G', 'T', 'G', 'A', 'G', 'A', 'C', 'C', 'T', 'T', 'G', 'T', 'C', 'T', 'C', 'A']\n",
      "seq2:  ['T', 'G', 'A', 'G', 'C', 'C', 'A', 'A', 'G', 'G', 'T', 'C', 'G', 'C', 'A', 'C', 'C', 'A', 'C', 'T', 'G', 'C', 'A', 'C', 'T', 'C', 'C', 'A', 'G', 'C', 'C', 'T', 'G', 'G', 'G', 'C', 'G', 'A', 'C', 'A', 'G', 'A', 'G', 'C', 'G', 'A', 'G', 'A', 'C', 'T', 'C', 'T', 'G', 'T', 'C', 'A']\n",
      "pred:  ['T', 'G', 'A', 'G', 'C', 'C', 'A', 'A', 'G', 'A', 'A', 'C', 'G', 'T', 'G', 'A', 'A', 'T', 'T', 'A', 'G', 'A', 'A', 'C', 'A', 'A', 'C', 'A', 'A', 'C', 'A', 'A', 'G', 'A', 'C', 'T', 'A', 'A', 'G', 'A', 'G', 'A', 'G', 'G', 'T', 'T', 'A', 'C', 'A', 'A', 'T', 'A', 'C', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "train loss:  0.00021262782684061676\n",
      "seq1:  ['T', 'T', 'T', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'G', 'A', 'G']\n",
      "pred:  ['C', 'T', 'G', 'A', 'G']\n",
      "train loss:  0.000285030750092119\n",
      "seq1:  ['A', 'A', 'A', 'A']\n",
      "seq2:  ['T', 'T', 'T', 'T']\n",
      "pred:  ['T', 'T', 'T', 'T']\n",
      "train loss:  0.23963351547718048\n",
      "seq1:  ['G', 'A', 'G', 'A', 'C', 'A', 'A', 'A', 'G', 'T', 'C']\n",
      "seq2:  ['G', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "pred:  ['G', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'G']\n",
      "train loss:  0.7751249670982361\n",
      "seq1:  ['G', 'G', 'T', 'G', 'G', 'T', 'G', 'G', 'T', 'A', 'G', 'C', 'T', 'T', 'A', 'T', 'C', 'T', 'G', 'C', 'A', 'A', 'A', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'G', 'C', 'C', 'A']\n",
      "seq2:  ['T', 'G', 'G', 'T', 'G', 'C', 'C', 'T', 'T', 'T', 'G', 'A', 'T', 'T', 'G', 'T', 'T', 'T', 'G', 'G', 'T', 'T', 'G', 'C', 'T', 'T', 'T', 'G', 'A', 'T', 'T']\n",
      "pred:  ['T', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'T', 'T', 'C', 'A', 'T', 'T', 'G', 'T', 'C', 'C', 'G', 'G', 'T', 'T', 'G', 'T', 'T', 'T', 'T', 'G', 'A', 'T', 'T']\n",
      "train loss:  0.01128531713038683\n",
      "seq1:  ['T', 'G', 'G', 'T', 'T', 'G']\n",
      "seq2:  ['C', 'A', 'A', 'G', 'C', 'A']\n",
      "pred:  ['C', 'A', 'A', 'G', 'C', 'A']\n",
      "train loss:  0.17926383018493652\n",
      "seq1:  ['G', 'C', 'T', 'G', 'C', 'T']\n",
      "seq2:  ['G', 'G', 'C', 'G', 'C']\n",
      "pred:  ['G', 'T', 'C', 'G', 'C', 'T']\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloaders, MAX_SEQ_LENGTH, vocab_size, vocabulary\n",
    "import torch.nn as nn\n",
    "import random\n",
    "batch_size = 1\n",
    "train_loader, dev_loader, test_loader = get_dataloaders(batch_size=batch_size)\n",
    "# random select 5 training samples\n",
    "random.seed(0)\n",
    "train_samples = random.sample(list(train_loader), 10)\n",
    "# random select 5 dev samples\n",
    "dev_samples = random.sample(list(dev_loader), 10)\n",
    "# random select 5 test samples\n",
    "test_samples = random.sample(list(test_loader), 5)\n",
    "\n",
    "vocab = list(vocabulary.keys())\n",
    "def outputs_to_seq(outputs):\n",
    "    outputs = outputs.argmax(dim=-1)\n",
    "    # print(outputs)\n",
    "    # 取vocab中的token\n",
    "    outputs = [vocab[i] for i in outputs]\n",
    "    # 去掉padding\n",
    "    if 'P' in outputs:\n",
    "        outputs = outputs[:outputs.index('P')]\n",
    "    if 'E' in outputs:\n",
    "        outputs = outputs[:outputs.index('E')]\n",
    "    return outputs\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 输出原来的seq1和seq2，还有预测的seq2\n",
    "for i in range(10):\n",
    "    seq1, seq2 = train_samples[i]\n",
    "    seq1 = seq1.to(device)\n",
    "    seq2 = seq2.to(device)\n",
    "    outputs = model(seq1.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs.reshape(MAX_SEQ_LENGTH, -1), seq2.reshape(MAX_SEQ_LENGTH, -1))\n",
    "    print(\"train loss: \", loss.item())\n",
    "    print(\"seq1: \", outputs_to_seq(seq1[0]))\n",
    "    print(\"seq2: \", outputs_to_seq(seq2[0]))\n",
    "    outputs = outputs_to_seq(outputs.reshape(MAX_SEQ_LENGTH, -1))\n",
    "    print(\"pred: \", outputs)\n",
    "\n",
    "print(\"dev samples\")\n",
    "for i in range(10):\n",
    "    seq1, seq2 = dev_samples[i]\n",
    "    seq1 = seq1.to(device)\n",
    "    seq2 = seq2.to(device)\n",
    "    outputs = model(seq1.to(device))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs.reshape(MAX_SEQ_LENGTH, -1), seq2.reshape(MAX_SEQ_LENGTH, -1))\n",
    "    print(\"train loss: \", loss.item())\n",
    "    print(\"seq1: \", outputs_to_seq(seq1[0]))\n",
    "    print(\"seq2: \", outputs_to_seq(seq2[0]))\n",
    "    outputs = outputs_to_seq(outputs.reshape(MAX_SEQ_LENGTH, -1))\n",
    "    print(\"pred: \", outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
