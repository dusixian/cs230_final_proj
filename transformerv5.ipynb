{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataloader import get_dataloaders, MAX_SEQ_LENGTH, vocab_size\n",
    "import time\n",
    "\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "['ADAR1_seq.txt', 'ADAR2_seq.txt', 'ADAR3_seq.txt', 'Endogenous_ADAR1_seq.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch [1/20], Loss: 1.3836\n",
      "Epoch 2/20\n",
      "Epoch [2/20], Loss: 1.3295\n",
      "Epoch 3/20\n",
      "Epoch [3/20], Loss: 1.2895\n",
      "Epoch 4/20\n",
      "Epoch [4/20], Loss: 1.2657\n",
      "Epoch 5/20\n",
      "Epoch [5/20], Loss: 1.2461\n",
      "Epoch 6/20\n",
      "Epoch [6/20], Loss: 1.2288\n",
      "Epoch 7/20\n",
      "Epoch [7/20], Loss: 1.2139\n",
      "Epoch 8/20\n",
      "Epoch [8/20], Loss: 1.2011\n",
      "Epoch 9/20\n",
      "Epoch [9/20], Loss: 1.1894\n",
      "Epoch 10/20\n",
      "Epoch [10/20], Loss: 1.1784\n",
      "Dev Loss: 1.1686\n",
      "Epoch 11/20\n",
      "Epoch [11/20], Loss: 1.1703\n",
      "Epoch 12/20\n",
      "Epoch [12/20], Loss: 1.1621\n",
      "Epoch 13/20\n",
      "Epoch [13/20], Loss: 1.1530\n",
      "Epoch 14/20\n",
      "Epoch [14/20], Loss: 1.1474\n",
      "Epoch 15/20\n",
      "Epoch [15/20], Loss: 1.1387\n",
      "Epoch 16/20\n",
      "Epoch [16/20], Loss: 1.1323\n",
      "Epoch 17/20\n",
      "Epoch [17/20], Loss: 1.1250\n",
      "Epoch 18/20\n",
      "Epoch [18/20], Loss: 1.1175\n",
      "Epoch 19/20\n",
      "Epoch [19/20], Loss: 1.1119\n",
      "Epoch 20/20\n",
      "Epoch [20/20], Loss: 1.1057\n",
      "Dev Loss: 1.1028\n",
      "Best dev loss: 1.1028, training loss: 1.1057\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "class RNAPairTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, feature_dim, num_layers=2, nhead=8, device='cpu'):\n",
    "        super(RNAPairTransformer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim # 输入维度，vocab_size\n",
    "        self.hidden_dim = hidden_dim # Transformer模型中每一层的特征向量维度\n",
    "        self.output_dim = output_dim  # 输出维度，vocab_size\n",
    "        self.feature_dim = feature_dim # feature维度\n",
    "        self.num_layers = num_layers # Transformer模型中encoder和decoder的层数\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer for one-hot encoded input\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim) # 每个碱基都有一个固定的特征向量表示\n",
    "        self.feature_embedding = nn.Linear(feature_dim, hidden_dim) # 每个碱基对都有一个固定的特征向量表示\n",
    "        self.concat_projection = nn.Linear(hidden_dim * 2, hidden_dim) # 将两个特征向量拼接后映射到hidden_dim维度\n",
    "        self.positional_encoding = self._generate_positional_encoding(MAX_SEQ_LENGTH, hidden_dim) # 位置编码\n",
    "\n",
    "        # Transformer Encoder-Decoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead, # 多头注意力机制的头数\n",
    "            num_encoder_layers=num_layers, # encoder层数\n",
    "            num_decoder_layers=num_layers, # decoder层数\n",
    "            dim_feedforward=hidden_dim * 4, # 前馈网络中隐层的维度\n",
    "            batch_first=True, # 输入数据的形状为(batch_size, seq_length, feature_dim)。\n",
    "            norm_first=True, # 加normalization\n",
    "         #   dropout=0.1, # dropout概率\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # 利用一个全连接层将隐藏层的特征向量映射到输出维度\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_length, hidden_dim):\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / hidden_dim)\n",
    "        )\n",
    "        positional_encoding = torch.zeros(seq_length, hidden_dim)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        return positional_encoding\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_features):\n",
    "        # Generate target mask\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        # Add positional encoding to embeddings\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, : src.size(1), :].to(self.device)\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, : tgt.size(1), :].to(self.device)\n",
    "        src_feat_emb = self.feature_embedding(src_features).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "        src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "        src_emb = self.concat_projection(src_emb_concat)\n",
    "        # Pass through Transformer\n",
    "        transformer_output = self.transformer(src_emb, tgt_emb, tgt_mask=self.tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(transformer_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, time_stamp):\n",
    "    best_model = None\n",
    "    best_dev_loss = float('inf')\n",
    "    best_train_loss = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    loss_arr = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for seq1, feature1, seq2, _ in train_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            feature1 = torch.stack(feature1, dim=1)\n",
    "            feature1 = feature1.to(device).float()\n",
    "\n",
    "            # Shift target sequence for decoder input\n",
    "            tgt_input = seq2[:, :-1]\n",
    "            tgt_output = seq2[:, 1:]\n",
    "\n",
    "            outputs = model(seq1, tgt_input, feature1)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) #apply gradient clipping\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "        loss_arr.append(total_loss)\n",
    "\n",
    "\n",
    "        # eval model\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            dev_loss = evaluate_model(model, dev_loader, criterion, device)\n",
    "            if dev_loss < best_dev_loss:\n",
    "                best_dev_loss = dev_loss\n",
    "                best_model = model\n",
    "                best_train_loss = total_loss\n",
    "                best_epoch = epoch\n",
    "    \n",
    "    # Save best model\n",
    "    if save_model:\n",
    "        torch.save(best_model.state_dict(), './model/'+time_stamp+'/transformer_model_best.pth')\n",
    "\n",
    "        import json\n",
    "        with open('./model/'+time_stamp+'/loss.json', 'w') as f:\n",
    "            json.dump({'loss': loss_arr, \n",
    "                    'best_epoch': best_epoch, \n",
    "                    'best_dev_loss': best_dev_loss, \n",
    "                    'best_train_loss': best_train_loss}, f, indent=4)\n",
    "            \n",
    "    print(f'Best dev loss: {best_dev_loss:.4f}, training loss: {best_train_loss:.4f}')\n",
    "    print('Training finished')\n",
    "\n",
    "def evaluate_model(model, dev_loader, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for seq1, feature1, seq2, _ in dev_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            feature1 = torch.stack(feature1, dim=1)\n",
    "            feature1 = feature1.to(device).float()\n",
    "\n",
    "            # Shift target sequence for decoder input\n",
    "            tgt_input = seq2[:, :-1]\n",
    "            tgt_output = seq2[:, 1:]\n",
    "\n",
    "            outputs = model(seq1, tgt_input, feature1)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Dev Loss: {total_loss / len(dev_loader):.4f}')\n",
    "    return total_loss / len(dev_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    input_dim = vocab_size\n",
    "    hidden_dim = 128\n",
    "    feature_dim = 4\n",
    "    output_dim = vocab_size\n",
    "    num_layers = 4\n",
    "    nhead = 8\n",
    "    num_epochs = 20\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 32\n",
    "    time_stamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    " \n",
    "    if save_model:\n",
    "        import os\n",
    "        if not os.path.exists('./model'):\n",
    "            os.makedirs('./model')\n",
    "        os.makedirs('./model/'+time_stamp)\n",
    "\n",
    "        import json\n",
    "        hyperparameters = {\n",
    "            'time_stamp': time_stamp,\n",
    "            'model': 'transformer',\n",
    "            'input_dim': input_dim,\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'feature_dim': feature_dim,\n",
    "            'output_dim': output_dim,\n",
    "            'num_layers': num_layers,\n",
    "            'nhead': nhead,\n",
    "            'num_epochs': num_epochs,\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        with open('./model/'+time_stamp+'/hyperparameters.json', 'w') as f:\n",
    "            json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "    # Device configuration\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print('Using ' + device)\n",
    "\n",
    "    # Load data\n",
    "    train_loader, dev_loader, test_loader = get_dataloaders(batch_size=batch_size, one_hot_encode=False, start_token=True, get_feature=True)\n",
    "\n",
    "    # Initialize model, criterion and optimizer\n",
    "    model = RNAPairTransformer(input_dim, hidden_dim, output_dim, feature_dim, num_layers, nhead, device).to(device)\n",
    "    weight = torch.tensor([1,1,1,1,2,0.01,1,1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** epoch)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, train_loader, criterion, optimizer, num_epochs, device, time_stamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequence with \n",
    "def generate_sequence(model, src, feature1, start_token, max_len, device):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    batch_size = src.size(0)\n",
    "\n",
    "    # Initialize\n",
    "    tgt = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len - 1): \n",
    "            tgt_emb = model.embedding(tgt) + model.positional_encoding[:, : tgt.size(1), :].to(device)\n",
    "            src_emb = model.embedding(src) + model.positional_encoding[:, : src.size(1), :].to(device)\n",
    "            src_feat_emb = model.feature_embedding(feature1).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "            src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "            src_emb = model.concat_projection(src_emb_concat)\n",
    "\n",
    "            # outputs from transformer\n",
    "            outputs = model.transformer(src_emb, tgt_emb)\n",
    "            logits = model.fc(outputs[:, -1, :])\n",
    "\n",
    "            # 贪心解码\n",
    "            next_token = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "    return tgt\n",
    "\n",
    "def greedy_decode_features(model, src, src_features, start_token, max_length, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        batch_size = src.size(0)\n",
    "        tgt = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)  # Initialize with start token\n",
    "        outputs = []\n",
    "\n",
    "        for _ in range(max_length-1):\n",
    "            # Get current predictions\n",
    "            tgt_emb = model.embedding(tgt) + model.positional_encoding[:, :tgt.size(1), :].to(device)\n",
    "            src_emb = model.embedding(src) + model.positional_encoding[:, :src.size(1), :].to(device)\n",
    "            src_feat_emb = model.feature_embedding(src_features).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "            src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "            src_emb = model.concat_projection(src_emb_concat)\n",
    "\n",
    "            tgt_mask = model._generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "            transformer_output = model.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "            logits = model.fc(transformer_output)  # Shape: (batch_size, tgt_len, vocab_size)\n",
    "\n",
    "            # Decode next token\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)  # Get the token with max probability\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        return tgt  # Remove start token from the output\n",
    "\n",
    "def greedy_decode_and_compute_loss(model, src, src_features, target, start_token, max_length, device):\n",
    "    model.eval()\n",
    "    weight = torch.tensor([1,1,1,1,2,0.01,1,1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight,reduction='none')\n",
    "    with torch.no_grad():\n",
    "        batch_size = src.size(0)\n",
    "        tgt = torch.full((batch_size, 1), start_token, dtype=torch.long, device=device)  # Initialize with start token\n",
    "        outputs = []\n",
    "        sequence_losses = torch.zeros(batch_size, device=device)  # To store loss for each sequence\n",
    "\n",
    "        for i in range(max_length - 1):\n",
    "            # Get current predictions\n",
    "            tgt_emb = model.embedding(tgt) + model.positional_encoding[:, :tgt.size(1), :].to(device)\n",
    "            src_emb = model.embedding(src) + model.positional_encoding[:, :src.size(1), :].to(device)\n",
    "            src_feat_emb = model.feature_embedding(src_features).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "            src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "            src_emb = model.concat_projection(src_emb_concat)\n",
    "\n",
    "            tgt_mask = model._generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "            transformer_output = model.transformer(src_emb, tgt_emb, tgt_mask=tgt_mask)\n",
    "            logits = model.fc(transformer_output)  # Shape: (batch_size, tgt_len, vocab_size)\n",
    "\n",
    "            # Calculate the cross-entropy loss for each token in the sequence\n",
    "            loss = criterion(logits[:, i, :].reshape(-1, logits.size(-1)), target[:, i].reshape(-1).long())  # Compare logits for each token with target\n",
    "            sequence_losses += loss  \n",
    "\n",
    "            # Decode next token using greedy strategy (argmax)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "\n",
    "        return tgt, sequence_losses/max_length  # Return the final decoded sequence and the loss for each sequence\n",
    "\n",
    "\n",
    "def greedy_decode_with_temperature(model, src, feature1, start_token, max_len,device, temperature=1.0):\n",
    "    src = src.to(device)\n",
    "    src = src.unsqueeze(0)  # Add batch dimension\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tgt = torch.tensor([start_token], device=device).unsqueeze(0)\n",
    "        src_emb = model.embedding(src) + model.positional_encoding[:, :src.size(1), :].to(device)\n",
    "        src_feat_emb = model.feature_embedding(feature1).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "        src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "        src_emb = model.concat_projection(src_emb_concat)\n",
    "        encoder_output = model.transformer.encoder(src_emb)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_emb = model.embedding(tgt) + model.positional_encoding[:, :tgt.size(1), :].to(device)\n",
    "            tgt_mask = model._generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "            decoder_output = model.transformer.decoder(tgt_emb, encoder_output, tgt_mask=tgt_mask)\n",
    "            logits = model.fc(decoder_output[:, -1, :])  # 最后一时间步\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)  # 调整概率分布\n",
    "            \n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()  # 随机采样\n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]], device=device)], dim=1)\n",
    "            \n",
    "        \n",
    "        return tgt\n",
    "\n",
    "def top_k_sampling(model, src, feature1, start_token, max_len,device, k=5):\n",
    "    src = src.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tgt = torch.tensor([start_token], device=device).unsqueeze(0)\n",
    "        src_emb = model.embedding(src) + model.positional_encoding[:, :src.size(1), :].to(device)\n",
    "        src_feat_emb = model.feature_embedding(feature1).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "        src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "        src_emb = model.concat_projection(src_emb_concat)\n",
    "        encoder_output = model.transformer.encoder(src_emb)\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            tgt_emb = model.embedding(tgt) + model.positional_encoding[:, :tgt.size(1), :].to(device)\n",
    "            tgt_mask = model._generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "            decoder_output = model.transformer.decoder(tgt_emb, encoder_output, tgt_mask=tgt_mask)\n",
    "            logits = model.fc(decoder_output[:, -1, :])\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            # 取 top-k 的概率和索引\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, k)\n",
    "            top_k_probs = top_k_probs / top_k_probs.sum()  # 归一化\n",
    "            next_token = torch.multinomial(top_k_probs, 1).item()\n",
    "            \n",
    "            tgt = torch.cat([tgt, torch.tensor([[next_token]], device=device)], dim=1)        \n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADAR1_seq.txt', 'ADAR2_seq.txt', 'ADAR3_seq.txt', 'Endogenous_ADAR1_seq.txt']\n",
      "train loss:  0.9240440130233765\n",
      "seq1:  ['G', 'G', 'C', 'C', 'C', 'A', 'G', 'A', 'C', 'T', 'G', 'G', 'C', 'A', 'C', 'C', 'T', 'G', 'A', 'A', 'G', 'A', 'T', 'G', 'C', 'C', 'T', 'A', 'A', 'G', 'A', 'T', 'A', 'A', 'A', 'A', 'A', 'T', 'G', 'C', 'C', 'C', 'A', 'A', 'G', 'A', 'T', 'C']\n",
      "seq2:  ['G', 'A', 'T', 'C', 'T', 'C', 'C', 'A', 'T', 'G', 'C', 'C', 'T', 'G', 'A', 'C', 'A', 'T', 'T', 'G', 'A', 'T', 'C', 'T', 'T', 'A', 'A', 'C', 'C', 'T', 'G', 'A', 'A', 'A', 'G', 'G', 'A', 'C', 'C', 'C', 'A', 'A', 'A', 'G', 'T', 'G', 'A', 'A', 'G', 'G', 'G', 'T', 'G', 'A', 'T', 'A', 'T', 'G', 'G', 'A', 'T', 'G', 'T', 'G', 'T', 'C', 'T', 'C', 'T', 'G', 'C', 'C']\n",
      "pred:  ['G', 'A', 'T', 'C', 'T', 'G', 'T', 'T', 'G', 'G', 'G', 'T', 'T', 'G', 'C', 'G', 'A', 'G', 'G', 'G', 'G', 'A', 'G', 'T', 'G', 'G', 'A', 'A', 'A', 'T', 'G', 'G', 'A', 'G', 'A', 'A', 'A', 'A', 'A']\n",
      "train loss:  0.5361067652702332\n",
      "seq1:  ['C', 'A', 'G', 'G', 'A', 'G', 'T', 'T', 'C', 'G', 'A', 'G', 'A', 'C', 'T', 'A', 'G', 'C', 'C', 'T', 'G', 'G', 'G', 'C', 'A', 'A', 'C', 'A', 'T', 'A', 'G', 'C', 'G', 'A', 'G']\n",
      "seq2:  ['T', 'T', 'C', 'T', 'A', 'T', 'A', 'C', 'C', 'T', 'T', 'G', 'C', 'C', 'A', 'G', 'G', 'T', 'A', 'A', 'A', 'T', 'A', 'T', 'G', 'A', 'T', 'G', 'A', 'A', 'A', 'C', 'T', 'C', 'A', 'C', 'A', 'G', 'C', 'T', 'G']\n",
      "pred:  ['C', 'T', 'T', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'T', 'T', 'C', 'T', 'G', 'C', 'C', 'G', 'T', 'A', 'A', 'T', 'A', 'T', 'T', 'A', 'T', 'T', 'A', 'A', 'A', 'T', 'T', 'T', 'T', 'T', 'T', 'A', 'T', 'G']\n",
      "train loss:  0.14509914815425873\n",
      "seq1:  ['G', 'A', 'G', 'T', 'C', 'T', 'C', 'T', 'C', 'G', 'G', 'T', 'C', 'A', 'C', 'T', 'C']\n",
      "seq2:  ['G', 'A', 'G', 'T', 'G', 'A', 'A', 'A', 'C', 'T', 'C', 'T', 'G', 'T', 'C', 'T', 'C']\n",
      "pred:  ['G', 'A', 'G', 'T', 'G', 'A', 'G', 'C', 'G', 'A', 'C']\n",
      "train loss:  0.4310867488384247\n",
      "seq1:  ['A', 'T', 'G', 'A', 'T', 'G', 'G', 'A', 'C', 'A', 'C', 'C', 'A', 'G', 'A', 'G', 'A', 'C', 'T', 'G', 'C', 'C', 'A', 'G', 'G', 'T', 'T', 'T', 'C', 'C', 'T', 'T', 'T', 'A', 'C', 'T']\n",
      "seq2:  ['A', 'G', 'T', 'T', 'T', 'C', 'C', 'C', 'T', 'G', 'A', 'A', 'A', 'C', 'C', 'T', 'G', 'G', 'G', 'C', 'T', 'C', 'T', 'T', 'G', 'A', 'A', 'G', 'A', 'C', 'G', 'C', 'A', 'T']\n",
      "pred:  ['A', 'G', 'T', 'A', 'A', 'G', 'A', 'A', 'A', 'G', 'G', 'G', 'G', 'A', 'T', 'T', 'G', 'G', 'A', 'A', 'T', 'G', 'T', 'G', 'G', 'G', 'A', 'A', 'T', 'A', 'A', 'T', 'A', 'T']\n",
      "train loss:  0.6309705972671509\n",
      "seq1:  ['A', 'G', 'G', 'G', 'A', 'T', 'G', 'A', 'A', 'G', 'G', 'G', 'A', 'G', 'A', 'G', 'T', 'T', 'T', 'T', 'C', 'T', 'T', 'A', 'A', 'A', 'G', 'A', 'A', 'G', 'G', 'T', 'C', 'C', 'C', 'A', 'A', 'A', 'G', 'T', 'T', 'G', 'A', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'T', 'G', 'T', 'C', 'G', 'C', 'T', 'T', 'T', 'A', 'G', 'G', 'A', 'T', 'C', 'A', 'A', 'C', 'T', 'T', 'T', 'A', 'C', 'C', 'T', 'G', 'T', 'A', 'C', 'C', 'T', 'T', 'T', 'T', 'C', 'T', 'C', 'C', 'T', 'T', 'T', 'C', 'C', 'T', 'C', 'C', 'C', 'T']\n",
      "pred:  ['C', 'T', 'T', 'C', 'A', 'T', 'T', 'T', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'T', 'T', 'G', 'G', 'A', 'T', 'T', 'T', 'T', 'A', 'T', 'T', 'T', 'T', 'T', 'A', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "train loss:  1.2373427152633667\n",
      "seq1:  ['T', 'C', 'G', 'T', 'T', 'C', 'T', 'G', 'T', 'T', 'G', 'C', 'C', 'C', 'A', 'G', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'T', 'G', 'C', 'A', 'A', 'C', 'C', 'T', 'C', 'C', 'A', 'C', 'C', 'T', 'C', 'C', 'C', 'A', 'G', 'G', 'T', 'T', 'C', 'A', 'A', 'G', 'T', 'G', 'A', 'T', 'T', 'C', 'T', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'T', 'A', 'G', 'C', 'C', 'T']\n",
      "seq2:  ['A', 'G', 'G', 'C', 'G', 'T', 'C', 'A', 'T', 'A', 'G', 'G', 'C', 'G', 'T', 'A', 'A', 'T', 'T', 'A', 'T', 'A', 'G', 'C', 'G', 'G', 'T', 'T', 'T', 'C', 'A', 'A', 'T', 'G', 'A', 'A', 'T', 'C', 'T', 'C', 'C', 'T', 'T', 'T', 'G', 'T', 'G', 'C', 'T', 'C', 'A', 'A', 'A', 'C', 'A', 'G', 'C', 'A', 'T', 'A', 'T', 'C', 'T', 'A', 'A', 'C', 'T', 'T', 'A', 'T', 'T', 'G', 'G', 'A', 'G', 'A', 'G', 'T', 'A', 'G', 'C', 'T', 'A', 'G', 'T', 'G', 'G', 'G']\n",
      "pred:  ['A', 'G', 'G', 'C', 'T', 'A', 'G', 'A', 'G', 'G', 'G', 'G', 'A', 'A', 'C', 'G', 'G', 'T', 'C', 'T', 'T', 'T', 'T', 'T', 'C', 'T', 'A', 'G', 'G', 'T', 'T', 'G', 'G', 'G', 'T', 'T', 'A', 'T', 'T', 'G', 'T', 'A', 'G', 'G', 'T', 'G', 'T', 'G', 'C', 'G', 'A', 'G', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'G', 'A', 'T', 'A', 'G', 'G', 'A', 'A', 'G', 'T', 'A', 'T', 'T', 'A', 'A', 'A', 'A', 'A', 'A', 'G', 'G', 'A', 'A', 'G', 'G', 'G', 'G', 'A', 'A']\n",
      "train loss:  0.5472033023834229\n",
      "seq1:  ['G', 'G', 'C', 'T', 'C', 'A', 'C', 'T', 'G', 'C', 'A', 'A', 'G', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'C', 'C', 'G', 'C', 'G', 'T', 'T', 'C', 'A', 'T', 'G', 'C', 'C', 'A', 'T', 'T', 'C', 'T', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'C', 'A', 'G', 'C', 'C', 'T', 'C', 'C', 'T', 'G', 'A', 'G', 'T', 'A', 'G', 'C', 'T', 'G', 'G', 'G', 'A', 'C', 'T', 'A', 'C', 'A', 'G', 'G', 'C', 'G', 'C', 'C', 'T', 'G']\n",
      "seq2:  ['C', 'A', 'G', 'A', 'A', 'C', 'T', 'C', 'C', 'T', 'G', 'A', 'T', 'G', 'T', 'C', 'T', 'C', 'A', 'A', 'G', 'T', 'G', 'A', 'T', 'C', 'C', 'A', 'C', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'G', 'G', 'C', 'C', 'T', 'C', 'C', 'C', 'G', 'A', 'A', 'G', 'T', 'G', 'C', 'T', 'G', 'G', 'G', 'A', 'T', 'T', 'A', 'G', 'A', 'G', 'G', 'C', 'A', 'T', 'G', 'A', 'G', 'C', 'C']\n",
      "pred:  ['C', 'A', 'G', 'G', 'G', 'G', 'T', 'G', 'T', 'T', 'G', 'T', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'G', 'A', 'A', 'G', 'A', 'T', 'C', 'C', 'T', 'C', 'C', 'T', 'A', 'C', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'C', 'C', 'A', 'A', 'G', 'G', 'T', 'G', 'C', 'T', 'G', 'G', 'G', 'A', 'T', 'T', 'A', 'C', 'G', 'G', 'C', 'C', 'A', 'C', 'G', 'C', 'G', 'C', 'C']\n",
      "train loss:  0.23920859396457672\n",
      "seq1:  ['T', 'T', 'A', 'G', 'A', 'T', 'G', 'G', 'A', 'G', 'T', 'C', 'T', 'C', 'G', 'C', 'T', 'C']\n",
      "seq2:  ['G', 'G', 'G', 'C', 'G', 'T', 'G', 'G', 'T', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'C', 'T', 'G', 'T', 'A', 'G']\n",
      "pred:  ['G', 'A', 'G', 'C', 'A', 'G', 'G', 'A', 'C', 'G', 'G', 'C', 'G', 'G', 'G', 'C', 'G', 'G', 'T', 'T', 'G', 'G', 'A', 'A']\n",
      "train loss:  0.53291255235672\n",
      "seq1:  ['A', 'G', 'C', 'A', 'C', 'T', 'T', 'T', 'G', 'A', 'G', 'A', 'G', 'G', 'C', 'T', 'G', 'A', 'G', 'G', 'C', 'G', 'A', 'G', 'T', 'G', 'G', 'A', 'T', 'C', 'G', 'C', 'C', 'T', 'G', 'A', 'G', 'C', 'T', 'C', 'A', 'G', 'G', 'A', 'G', 'T', 'T', 'C', 'G', 'A']\n",
      "seq2:  ['T', 'C', 'G', 'G', 'G', 'C', 'G', 'C', 'C', 'A', 'A', 'G', 'C', 'G', 'C', 'G', 'G', 'G', 'G', 'C', 'C', 'G', 'G', 'A', 'G', 'C', 'G', 'G', 'C', 'C', 'T', 'T', 'C', 'C', 'C', 'G', 'G', 'A', 'G', 'T', 'C', 'C', 'T']\n",
      "pred:  ['T', 'C', 'G', 'A', 'A', 'C', 'T', 'G', 'G', 'A', 'G', 'G', 'G', 'G', 'C', 'G', 'C', 'C', 'C', 'C', 'G', 'C', 'G', 'G', 'G', 'G', 'C', 'G', 'C', 'G', 'G', 'G', 'G', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'T', 'C', 'C', 'G']\n",
      "train loss:  0.48788973689079285\n",
      "seq1:  ['G', 'G', 'G', 'G', 'T', 'T', 'C', 'A', 'G', 'G', 'G', 'C', 'G', 'G', 'G', 'T', 'G', 'A', 'A', 'G', 'T', 'C', 'G', 'G', 'G', 'C', 'T', 'T', 'G', 'T', 'G', 'A', 'G', 'A', 'G', 'C', 'G', 'C', 'C', 'G', 'C']\n",
      "seq2:  ['G', 'C', 'G', 'C', 'G', 'C', 'G', 'C', 'A', 'C', 'C', 'C', 'C', 'T', 'T', 'C', 'C', 'C', 'C', 'C', 'A', 'C', 'C', 'G', 'A', 'G', 'C', 'G', 'C', 'G', 'C', 'G', 'C', 'G', 'T', 'T', 'C', 'C', 'C', 'T']\n",
      "pred:  ['G', 'C', 'G', 'G', 'C', 'C', 'C', 'C', 'C', 'G', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'G', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'G', 'C', 'C', 'C']\n",
      "dev samples\n",
      "dev loss:  0.03521818667650223\n",
      "seq1:  ['A', 'C', 'A', 'A', 'A', 'A', 'C', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'G', 'T', 'T', 'G', 'T']\n",
      "pred:  ['C', 'T', 'G', 'T', 'T', 'T', 'T']\n",
      "dev loss:  1.2433065176010132\n",
      "seq1:  ['A', 'G', 'T', 'C', 'T', 'T', 'G', 'C', 'G', 'A', 'G', 'T', 'G', 'G', 'A', 'C', 'C', 'A', 'C', 'T', 'G', 'G', 'C', 'G', 'T', 'C', 'C', 'C', 'T', 'C', 'T', 'G', 'A', 'A', 'G', 'G', 'A', 'A', 'A', 'A', 'G', 'C', 'A', 'A', 'G', 'C', 'A', 'G', 'G', 'A', 'G', 'G', 'T', 'G', 'G', 'T', 'G', 'T', 'G', 'T', 'C', 'C', 'G', 'T', 'G', 'G', 'A', 'G', 'A', 'T', 'G', 'C', 'A', 'G', 'A', 'A', 'A', 'T', 'A', 'G', 'T', 'G', 'T', 'G', 'G', 'T', 'C', 'C', 'T', 'A', 'G', 'G', 'G', 'C', 'T', 'G', 'A']\n",
      "seq2:  ['T', 'T', 'G', 'G', 'C', 'C', 'A', 'G', 'T', 'C', 'C', 'C', 'G', 'G', 'C', 'G', 'T', 'G', 'T', 'T', 'T', 'C', 'T', 'C', 'C', 'C', 'C', 'A', 'G', 'A', 'C', 'A', 'A', 'T', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'C', 'A', 'G', 'C', 'C', 'C', 'T', 'G', 'C', 'T', 'G', 'G', 'C', 'T', 'G', 'T', 'C', 'A', 'G', 'A', 'A', 'G', 'C', 'A', 'G', 'G', 'A', 'A', 'A', 'G', 'C', 'A', 'C', 'G', 'C', 'T', 'C', 'G', 'G', 'T', 'G', 'G', 'G', 'A', 'A', 'G', 'C', 'T', 'T', 'G', 'C', 'G', 'G', 'A', 'C', 'T']\n",
      "pred:  ['T', 'C', 'A', 'G', 'C', 'C', 'A', 'G', 'G', 'G', 'T', 'T', 'A', 'G', 'C', 'C', 'G', 'G', 'G', 'G', 'G', 'G', 'C', 'G', 'C', 'C', 'C', 'C', 'G', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'T', 'G', 'G', 'C', 'G', 'C', 'T', 'G', 'C', 'C', 'T', 'T', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'G', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'G', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'A', 'A', 'G', 'A', 'G', 'C']\n",
      "dev loss:  0.04070943593978882\n",
      "seq1:  ['G', 'C', 'A', 'G', 'A']\n",
      "seq2:  ['T', 'C', 'G', 'G', 'C']\n",
      "pred:  ['T', 'C', 'T', 'G', 'C']\n",
      "dev loss:  0.9017024040222168\n",
      "seq1:  ['G', 'C', 'T', 'T', 'G', 'A', 'G', 'C', 'C', 'C', 'A', 'G', 'G', 'G', 'A', 'G', 'G', 'T', 'C', 'A', 'A', 'G', 'G', 'T', 'G', 'G', 'C', 'A', 'G', 'T', 'G', 'A', 'A', 'C', 'C', 'G', 'T', 'G', 'A', 'C', 'C', 'A', 'T', 'G', 'C', 'C', 'A', 'C', 'T', 'G', 'C', 'A', 'C', 'T', 'C', 'C', 'A', 'G', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'G', 'G', 'G', 'T', 'G', 'A', 'C', 'G', 'G', 'A', 'G', 'T', 'G', 'A', 'A', 'A', 'G', 'C', 'T']\n",
      "seq2:  ['G', 'G', 'T', 'T', 'T', 'T', 'C', 'T', 'C', 'T', 'T', 'G', 'G', 'G', 'T', 'C', 'C', 'T', 'T', 'T', 'T', 'C', 'C', 'G', 'T', 'G', 'C', 'C', 'G', 'T', 'C', 'C', 'C', 'G', 'C', 'G', 'A', 'C', 'T', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'T', 'G', 'G', 'C', 'C', 'G', 'C', 'G', 'C', 'G', 'T', 'G', 'T', 'C', 'T', 'G', 'G', 'C', 'T', 'G', 'C']\n",
      "pred:  ['A', 'G', 'C', 'T', 'T', 'T', 'T', 'A', 'T', 'C', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'G', 'G', 'T', 'T', 'T', 'G', 'G', 'G', 'T', 'T', 'G', 'G', 'T', 'T', 'A', 'G', 'C', 'G', 'G', 'C', 'G', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'G', 'C', 'G', 'C', 'T', 'C', 'C', 'C', 'C', 'C', 'G', 'C', 'G', 'T', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "dev loss:  0.77034991979599\n",
      "seq1:  ['G', 'G', 'A', 'T', 'G', 'T', 'G', 'C', 'T', 'G', 'A', 'C', 'C', 'C', 'C', 'T', 'G', 'C', 'G', 'A', 'T', 'T', 'T', 'C', 'C', 'C', 'C', 'A', 'A', 'A', 'T', 'G', 'T', 'G', 'G', 'G', 'A', 'A', 'A', 'C', 'T', 'C', 'G', 'A', 'C', 'T', 'G', 'C', 'A', 'T', 'A', 'A', 'T', 'T', 'T', 'G', 'T', 'G', 'G', 'T', 'A']\n",
      "seq2:  ['T', 'A', 'C', 'C', 'T', 'G', 'G', 'C', 'A', 'G', 'G', 'G', 'G', 'A', 'G', 'A', 'T', 'A', 'C', 'C', 'A', 'T', 'G', 'A', 'T', 'C', 'A', 'C', 'G', 'A', 'A', 'G', 'G', 'T', 'G', 'G', 'T', 'T', 'T', 'T', 'C', 'C', 'C', 'A', 'G', 'G', 'G', 'C', 'G', 'A', 'G', 'G', 'C', 'T', 'T', 'A', 'T', 'C', 'C']\n",
      "pred:  ['T', 'A', 'C', 'A', 'A', 'G', 'A', 'A', 'A', 'G', 'G', 'A', 'A', 'A', 'G', 'G', 'G', 'G', 'G', 'A', 'A', 'G', 'G', 'G', 'G', 'G', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'A', 'G', 'G', 'A', 'G', 'G', 'G', 'G', 'A', 'A', 'A', 'G', 'G', 'A', 'A', 'A', 'G', 'G', 'G', 'G', 'A', 'G', 'G', 'G', 'G', 'C']\n",
      "dev loss:  0.5274980664253235\n",
      "seq1:  ['T', 'C', 'G', 'G', 'G', 'G', 'A', 'T', 'T', 'G', 'A', 'G', 'A', 'T', 'T', 'T', 'T', 'T', 'G', 'G', 'G', 'G', 'A', 'T', 'G', 'A', 'T', 'T', 'T', 'C', 'C', 'A', 'T', 'T', 'T', 'T', 'T', 'T', 'A', 'A']\n",
      "seq2:  ['T', 'T', 'A', 'A', 'A', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'A', 'G', 'T', 'G', 'T', 'A', 'T', 'G', 'A', 'T', 'C', 'A', 'G', 'G', 'G', 'T', 'C', 'G', 'G', 'A', 'T', 'G', 'C', 'G', 'T', 'C', 'C', 'T', 'G', 'G']\n",
      "pred:  ['T', 'T', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'T', 'A', 'T', 'A', 'A', 'T', 'A', 'T', 'A', 'A', 'A', 'A', 'A', 'A']\n",
      "dev loss:  0.38115087151527405\n",
      "seq1:  ['G', 'T', 'G', 'T', 'G', 'T', 'G', 'G', 'T', 'G', 'T', 'T', 'T', 'G', 'T', 'G', 'G', 'T', 'G', 'G', 'T', 'G', 'T', 'G', 'T']\n",
      "seq2:  ['A', 'C', 'T', 'G', 'G', 'A', 'G', 'G', 'T', 'G', 'T', 'T', 'T', 'T', 'G', 'T', 'G', 'A', 'G', 'T', 'T', 'A', 'C', 'T', 'G', 'G', 'C']\n",
      "pred:  ['A', 'C', 'A', 'G', 'A', 'C', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'T', 'T', 'T', 'G', 'T', 'G', 'A', 'G', 'T', 'T']\n",
      "dev loss:  0.8246549367904663\n",
      "seq1:  ['G', 'T', 'C', 'C', 'C', 'A', 'G', 'T', 'A', 'G', 'C', 'C', 'A', 'A', 'G', 'G', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'T', 'T', 'G', 'G', 'A', 'G', 'G', 'T', 'G', 'G', 'G', 'A', 'G', 'C', 'T', 'T', 'C', 'C', 'C', 'A', 'C', 'A', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'G', 'C']\n",
      "seq2:  ['G', 'C', 'T', 'G', 'C', 'C', 'A', 'T', 'C', 'C', 'A', 'T', 'G', 'T', 'G', 'C', 'T', 'A', 'G', 'T', 'C', 'A', 'C', 'T', 'G', 'G', 'C', 'A', 'G', 'C', 'C', 'T', 'G', 'C', 'A', 'C', 'C', 'T', 'G', 'G', 'T', 'G', 'G', 'G', 'T', 'G', 'G', 'T', 'G', 'T', 'C', 'C', 'T', 'G', 'A', 'A', 'G', 'C', 'T', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'C']\n",
      "pred:  ['G', 'C', 'A', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'C', 'G', 'G', 'G', 'G', 'G', 'C', 'G', 'G', 'G', 'G', 'C', 'G', 'C', 'G', 'C', 'G', 'C', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'G', 'C', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'C', 'T', 'G', 'G', 'G', 'G', 'G', 'A', 'G', 'G', 'A', 'G', 'G', 'G', 'G', 'G']\n",
      "dev loss:  0.7964612245559692\n",
      "seq1:  ['G', 'A', 'G', 'T', 'T', 'G', 'T', 'C', 'A', 'A', 'G', 'G', 'T', 'A', 'G', 'G', 'T', 'C', 'C', 'T', 'T', 'C', 'T', 'G', 'A', 'G', 'G', 'A', 'G', 'G', 'G', 'A', 'A', 'G', 'A', 'G', 'G', 'A', 'A', 'T', 'C', 'T', 'G', 'A', 'G', 'A', 'A', 'A', 'A', 'G', 'G']\n",
      "seq2:  ['C', 'C', 'T', 'A', 'G', 'G', 'T', 'T', 'T', 'T', 'T', 'G', 'T', 'C', 'C', 'T', 'C', 'T', 'T', 'C', 'A', 'T', 'A', 'T', 'C', 'C', 'C', 'C', 'C', 'G', 'G', 'T', 'G', 'A', 'C', 'C', 'C', 'A', 'G', 'G', 'A', 'T', 'T', 'G', 'G', 'C', 'C', 'C', 'A', 'C', 'A', 'C', 'A', 'T', 'A', 'A', 'T', 'C', 'T', 'C']\n",
      "pred:  ['C', 'C', 'T', 'T', 'T', 'G', 'A', 'T', 'T', 'T', 'T', 'T', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'C', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'G', 'T', 'T', 'T', 'T', 'T', 'T', 'G', 'G', 'A', 'G', 'T', 'T', 'T', 'A', 'T', 'T', 'T', 'G', 'T', 'G', 'T', 'T', 'G', 'T', 'G', 'T', 'T', 'T']\n",
      "dev loss:  0.05616053193807602\n",
      "seq1:  ['G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'T', 'A', 'G', 'A', 'A', 'G']\n",
      "seq2:  ['T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "pred:  ['C', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "test samples\n",
      "test loss: 1.2537609338760376\n",
      "seq1:  ['C', 'T', 'T', 'C', 'A', 'A', 'G', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'A', 'C', 'T', 'T', 'G', 'G', 'T', 'G', 'T', 'T', 'C', 'G', 'C', 'T', 'A', 'A', 'G', 'A', 'T', 'G', 'A', 'A', 'G', 'G', 'G', 'C', 'T', 'A', 'C', 'C', 'C', 'T', 'C', 'A', 'C', 'T', 'G', 'G', 'C', 'C', 'T', 'G', 'C', 'C', 'A', 'G', 'G', 'G', 'T']\n",
      "seq2:  ['G', 'C', 'C', 'C', 'T', 'A', 'G', 'A', 'G', 'G', 'A', 'G', 'C', 'T', 'G', 'G', 'G', 'A', 'A', 'C', 'C', 'C', 'T', 'G', 'C', 'A', 'G', 'G', 'T', 'G', 'A', 'C', 'C', 'T', 'C', 'T', 'C', 'A', 'G', 'A', 'T', 'C', 'C', 'T', 'C', 'C', 'A', 'G', 'A', 'A', 'G']\n",
      "pred:  ['A', 'C', 'C', 'C', 'T', 'G', 'G', 'G', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']\n",
      "test loss: 0.8893924951553345\n",
      "seq1:  ['G', 'A', 'G', 'G', 'T', 'G', 'T', 'C', 'A', 'G', 'A', 'A']\n",
      "seq2:  ['T', 'T', 'C', 'T', 'G', 'T', 'T', 'C', 'A', 'C', 'A', 'A', 'G', 'C']\n",
      "pred:  ['T', 'T', 'T', 'T', 'G', 'A', 'G', 'A', 'C', 'A', 'G', 'T', 'C']\n",
      "test loss: 1.4109907150268555\n",
      "seq1:  ['G', 'C', 'C', 'C', 'C', 'T', 'G', 'A', 'G', 'A', 'T', 'A', 'A', 'A', 'G', 'A', 'G', 'G', 'A', 'G', 'C', 'A', 'T', 'G', 'T', 'G', 'G', 'A', 'G', 'C', 'A', 'C', 'C', 'C', 'G', 'G', 'G', 'G', 'C', 'T', 'C', 'T', 'G', 'C', 'T', 'C', 'C', 'A', 'G', 'C', 'C', 'C', 'C', 'T', 'C', 'C', 'T', 'A', 'G', 'A', 'G', 'C', 'G', 'G', 'A', 'A', 'T', 'G', 'T', 'C', 'C', 'T', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'G', 'G', 'A', 'C', 'A', 'A', 'C', 'C', 'T', 'G', 'C', 'C', 'C', 'T', 'G', 'G', 'G', 'T', 'T', 'G', 'G', 'G', 'A', 'C', 'C', 'C', 'G', 'G', 'A', 'G', 'T', 'C', 'C', 'T', 'G', 'C', 'C', 'C', 'T', 'G', 'A', 'C', 'A', 'C', 'C', 'C', 'C', 'C', 'G', 'T', 'C', 'C', 'C', 'T', 'C', 'T', 'A', 'G', 'A', 'G', 'A', 'C', 'T', 'A', 'T', 'G', 'G', 'C']\n",
      "pred:  ['C', 'T', 'G', 'G', 'G', 'C', 'T', 'G', 'G', 'G', 'C', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
      "test loss: 2.1735012531280518\n",
      "seq1:  ['C', 'A', 'G', 'G', 'G', 'G', 'A', 'G', 'G', 'C', 'T', 'G', 'A', 'G', 'G', 'C', 'A', 'G', 'G', 'A', 'G', 'A', 'A', 'T', 'C', 'G', 'C', 'T', 'T', 'G', 'A', 'A', 'C', 'C', 'C', 'G', 'G', 'G', 'A', 'G', 'G', 'T', 'G', 'T', 'A', 'G', 'G', 'T', 'T', 'G', 'C', 'A', 'G', 'T', 'G', 'A', 'G', 'C', 'C', 'G', 'A', 'G', 'A', 'T', 'T', 'G', 'C']\n",
      "seq2:  ['G', 'T', 'G', 'G', 'C', 'C', 'T', 'G', 'G', 'C', 'T', 'T', 'C', 'C', 'T', 'C', 'C', 'G', 'G', 'G', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'C', 'T', 'T', 'G', 'G', 'G', 'C', 'C', 'G', 'G', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'G', 'G', 'G', 'A', 'G', 'C', 'A', 'C', 'G', 'G', 'A', 'G', 'C', 'T', 'G', 'G', 'C', 'C', 'T', 'T', 'T', 'G', 'C', 'T', 'G', 'G', 'G', 'G', 'C', 'G', 'A', 'C', 'T', 'C', 'C', 'T', 'C', 'C', 'C', 'C', 'T', 'G']\n",
      "pred:  ['G', 'C', 'A', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T', 'T']\n",
      "test loss: 1.1598199605941772\n",
      "seq1:  ['A', 'C', 'C', 'A', 'C', 'A', 'G', 'G', 'G', 'A', 'G', 'C', 'A', 'G', 'T', 'A', 'C', 'C', 'T', 'G', 'C', 'C', 'T', 'A', 'T', 'C', 'T', 'G', 'C', 'T', 'G', 'G', 'A', 'C', 'A', 'G', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'C']\n",
      "seq2:  ['G', 'C', 'G', 'T', 'T', 'T', 'T', 'C', 'G', 'C', 'A', 'C', 'T', 'C', 'C', 'A', 'G', 'C', 'G', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'C', 'T', 'G', 'G', 'C', 'G', 'G', 'C', 'T', 'C', 'T', 'G', 'C', 'G', 'G', 'C', 'C', 'G', 'T', 'C', 'A', 'C', 'C', 'A', 'T', 'G', 'G', 'T']\n",
      "pred:  ['G', 'T', 'C', 'C', 'T', 'C', 'T', 'C', 'T', 'G', 'C', 'T', 'G', 'G', 'C', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
      "test loss: 0.9682850241661072\n",
      "seq1:  ['C', 'T', 'T', 'A', 'C', 'C', 'T', 'T', 'C', 'T', 'A', 'T', 'G', 'G', 'A', 'G', 'G', 'C', 'A', 'G', 'G', 'G', 'G', 'C', 'G', 'G', 'T', 'G', 'G', 'C', 'C', 'G', 'G', 'C', 'T', 'C', 'C', 'A', 'C', 'A', 'T', 'G', 'C', 'T', 'G', 'A', 'T', 'G', 'G', 'G', 'C', 'A', 'C', 'C', 'A', 'C', 'T', 'G', 'A', 'A', 'G', 'A', 'A', 'C', 'C', 'T']\n",
      "seq2:  ['G', 'G', 'G', 'G', 'C', 'T', 'G', 'G', 'G', 'C', 'C', 'T', 'G', 'C', 'T', 'G', 'G', 'C', 'C', 'G', 'C', 'G', 'T', 'G', 'G', 'G', 'G', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'T', 'G', 'C', 'T', 'G', 'A', 'C', 'T', 'G', 'A', 'T', 'A', 'T', 'T', 'A', 'A', 'G', 'A', 'G', 'G', 'G', 'A', 'A', 'A', 'G']\n",
      "pred:  ['A', 'G', 'G', 'T', 'T', 'T', 'T', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G', 'G']\n",
      "test loss: 1.247341513633728\n",
      "seq1:  ['G', 'G', 'T', 'A', 'A', 'A', 'T', 'T', 'A', 'T', 'T', 'G', 'G', 'A', 'T', 'A', 'T', 'A', 'A', 'A', 'G', 'G', 'T', 'C', 'T', 'A', 'G', 'A', 'G', 'T', 'T', 'C', 'A', 'G']\n",
      "seq2:  ['C', 'T', 'G', 'G', 'A', 'C', 'T', 'T', 'G', 'G', 'C', 'T', 'C', 'C', 'A', 'G', 'A', 'G', 'A', 'C', 'C', 'T', 'G', 'A', 'G', 'T', 'T', 'T', 'G', 'A', 'A', 'T', 'C', 'C', 'C', 'A', 'G', 'C', 'T', 'C', 'G', 'G', 'C', 'C']\n",
      "pred:  ['C', 'T', 'G', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A']\n",
      "test loss: 1.9464536905288696\n",
      "seq1:  ['G', 'C', 'A', 'C', 'T', 'T', 'T', 'G', 'A', 'T', 'T', 'A', 'C', 'A', 'G', 'G', 'C', 'A', 'T', 'G', 'A', 'G', 'C', 'C', 'A', 'C', 'T', 'G', 'T', 'G', 'C', 'T', 'T', 'G', 'G', 'C', 'C']\n",
      "seq2:  ['G', 'G', 'C', 'T', 'G', 'G', 'G', 'C', 'G', 'C', 'T', 'G', 'T', 'G', 'G', 'C', 'T', 'C', 'A', 'C', 'G', 'C', 'C', 'T', 'G', 'T', 'A', 'A', 'T', 'C', 'C', 'C', 'A', 'G', 'C']\n",
      "pred:  ['G', 'G', 'C', 'C', 'A', 'G', 'G', 'C', 'T', 'G', 'G', 'A', 'G', 'T', 'G', 'C', 'A', 'G', 'T', 'G', 'G', 'C', 'A', 'G', 'T', 'G', 'G', 'C']\n",
      "test loss: 0.6064359545707703\n",
      "seq1:  ['G', 'C', 'T', 'C', 'C', 'T', 'A', 'T', 'A', 'C', 'G', 'T', 'T', 'A', 'G', 'G', 'G', 'C', 'A', 'G', 'T', 'C', 'A', 'C', 'T', 'G', 'A', 'A']\n",
      "seq2:  ['T', 'T', 'C', 'A', 'G', 'A', 'G', 'A', 'A', 'A', 'C', 'T', 'G', 'C', 'T', 'G', 'A', 'G', 'T']\n",
      "pred:  ['T', 'T', 'C', 'A', 'G', 'G', 'A', 'G', 'A', 'C', 'A', 'G', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'A', 'G', 'C']\n",
      "test loss: 1.3579843044281006\n",
      "seq1:  ['G', 'T', 'G', 'T', 'G', 'T', 'G', 'T', 'A', 'T', 'G', 'T', 'G', 'T', 'G', 'T', 'G', 'G', 'T']\n",
      "seq2:  ['G', 'C', 'T', 'C', 'T', 'T', 'G', 'G', 'A', 'G', 'G', 'G', 'C', 'A', 'A', 'G', 'G', 'A', 'C']\n",
      "pred:  ['A', 'C', 'C', 'A', 'C', 'A', 'C', 'A', 'C', 'A', 'C', 'A', 'C', 'A', 'C']\n"
     ]
    }
   ],
   "source": [
    "from dataloader import get_dataloaders, MAX_SEQ_LENGTH, vocab_size, vocabulary\n",
    "import torch.nn as nn\n",
    "import random\n",
    "batch_size = 1\n",
    "train_loader, dev_loader, test_loader = get_dataloaders(batch_size=batch_size, one_hot_encode=False, start_token=True, get_feature=True)\n",
    "random.seed(0)\n",
    "train_samples = random.sample(list(train_loader), 10)\n",
    "# random select 5 dev samples\n",
    "dev_samples = random.sample(list(dev_loader), 10)\n",
    "# random select 5 test samples\n",
    "test_samples = random.sample(list(test_loader), 10)\n",
    "\n",
    "vocab = list(vocabulary.keys())\n",
    "def outputs_to_seq(outputs, flag=False):\n",
    "    if flag:\n",
    "        #print(outputs)\n",
    "        outputs = outputs.argmax(dim=-1)\n",
    "    # print(outputs.shape)\n",
    "    outputs = [vocab[i] for i in outputs]\n",
    "    if 'S' in outputs:\n",
    "        outputs = outputs[1:]\n",
    "    if 'P' in outputs:\n",
    "        outputs = outputs[:outputs.index('P')]\n",
    "    if 'E' in outputs:\n",
    "        outputs = outputs[:outputs.index('E')]\n",
    "    return outputs\n",
    "\n",
    "model.eval()\n",
    "# 输出原来的seq1和seq2，还有预测的seq2\n",
    "for i in range(10):\n",
    "    seq1, feature1, seq2, _ = train_samples[i]\n",
    "    seq1 = seq1.to(device)\n",
    "    seq2 = seq2.to(device)\n",
    "    feature1 = torch.stack(feature1, dim=1)\n",
    "    feature1 = feature1.to(device).float()\n",
    "    tgt_input = seq2[:, :-1]\n",
    "    tgt_output = seq2[:, 1:]\n",
    "    outputs = model(seq1, tgt_input, feature1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "    print(\"train loss: \", loss.item())\n",
    "    print(\"seq1: \", outputs_to_seq(seq1[0][1:]))\n",
    "    print(\"seq2: \", outputs_to_seq(seq2[0][1:]))\n",
    "    print(\"pred: \", outputs_to_seq(outputs[0], True))\n",
    "\n",
    "print(\"dev samples\")\n",
    "for i in range(10):\n",
    "    seq1, feature1, seq2, _ = dev_samples[i]\n",
    "    seq1 = seq1.to(device)\n",
    "    seq2 = seq2.to(device)\n",
    "    feature1 = torch.stack(feature1, dim=1)\n",
    "    feature1 = feature1.to(device).float()\n",
    "    tgt_input = seq2[:, :-1]\n",
    "    tgt_output = seq2[:, 1:]\n",
    "    outputs = model(seq1, tgt_input, feature1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "    print(\"dev loss: \", loss.item())\n",
    "    print(\"seq1: \", outputs_to_seq(seq1[0][1:]))\n",
    "    print(\"seq2: \", outputs_to_seq(seq2[0][1:]))\n",
    "    print(\"pred: \", outputs_to_seq(outputs[0], True))\n",
    "\n",
    "#test model\n",
    "print(\"test samples\")\n",
    "generation_method = 'features_loss' # change to 'greedy' or 'greedy_temp' if necessary\n",
    "for i in range(10):\n",
    "    seq1, feature1, seq2, _ = test_samples[i]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "        feature1 = torch.stack(feature1, dim=1)\n",
    "        feature1 = feature1.to(device).float()\n",
    "        # Generate sequences\n",
    "        if generation_method == 'greedy_temp':\n",
    "            generated_seq = greedy_decode_with_temperature(model, seq1, feature1, 7, MAX_SEQ_LENGTH, device,1.2)\n",
    "        if generation_method == 'greedy':\n",
    "            generated_seq = generate_sequence(model, seq1, feature1, 7, MAX_SEQ_LENGTH, device)\n",
    "        if generation_method == 'k_sampling':\n",
    "            generated_seq = top_k_sampling(model, seq1, feature1, 7, MAX_SEQ_LENGTH, device,2)\n",
    "        if generation_method=='features':\n",
    "            generated_seq=greedy_decode_features(model, seq1, feature1, 7, MAX_SEQ_LENGTH, device)\n",
    "        if generation_method=='features_loss':\n",
    "            generated_seq,loss=greedy_decode_and_compute_loss(model, seq1, feature1, seq2,7, MAX_SEQ_LENGTH, device)\n",
    "        # Compute loss\n",
    "\n",
    "        for j, (sequence, tmp_loss,tmp_seq1,tmp_seq2) in enumerate(zip(generated_seq, loss,seq1,seq2)):\n",
    "            print(\"test loss:\", tmp_loss.item())\n",
    "            print(\"seq1: \", outputs_to_seq(tmp_seq1[1:]))\n",
    "            print(\"seq2: \", outputs_to_seq(tmp_seq2[1:]))\n",
    "            print(\"pred: \", outputs_to_seq(sequence))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
