{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataloader import get_dataloaders, MAX_SEQ_LENGTH, vocab_size\n",
    "import time\n",
    "\n",
    "save_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNAPairTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, feature_dim, num_layers=2, nhead=8, device='cpu'):\n",
    "        super(RNAPairTransformer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim # input size，equal to vocab_size\n",
    "        self.hidden_dim = hidden_dim # hidden size\n",
    "        self.output_dim = output_dim  # output size，vocab_size\n",
    "        self.feature_dim = feature_dim # feature size\n",
    "        self.num_layers = num_layers # number of layers in the Transformer\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer for one-hot encoded input\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim) # Embed the RNA sequence\n",
    "        self.feature_embedding = nn.Linear(feature_dim, hidden_dim) # Embed the RNA features\n",
    "        self.concat_projection = nn.Linear(hidden_dim * 2, hidden_dim) # Project the two embedding vector to hidden size\n",
    "        self.positional_encoding = self._generate_positional_encoding(MAX_SEQ_LENGTH, hidden_dim) # positional encoding\n",
    "\n",
    "        # Transformer Encoder-Decoder\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=nhead, # number of heads for multihead attention\n",
    "            num_encoder_layers=num_layers, # layer of encoder\n",
    "            num_decoder_layers=num_layers, # layer of decoder\n",
    "            dim_feedforward=hidden_dim * 4, # hidden size\n",
    "            batch_first=True, # size of the input is (batch_size, seq_length, feature_dim)。\n",
    "            norm_first=True, # normalization\n",
    "         #   dropout=0.1, # dropout threshold \n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # project the hidden size to the output size\n",
    "\n",
    "    def _generate_positional_encoding(self, seq_length, hidden_dim):\n",
    "        position = torch.arange(0, seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, hidden_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / hidden_dim)\n",
    "        )\n",
    "        positional_encoding = torch.zeros(seq_length, hidden_dim)\n",
    "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        positional_encoding = positional_encoding.unsqueeze(0)\n",
    "        return positional_encoding\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, src, tgt, src_features):\n",
    "        # Generate target mask\n",
    "        self.tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
    "        # Add positional encoding to embeddings\n",
    "        src_emb = self.embedding(src) + self.positional_encoding[:, : src.size(1), :].to(self.device)\n",
    "        tgt_emb = self.embedding(tgt) + self.positional_encoding[:, : tgt.size(1), :].to(self.device)\n",
    "        src_feat_emb = self.feature_embedding(src_features).unsqueeze(1).expand(-1, src_emb.size(1), -1)\n",
    "        src_emb_concat = torch.cat([src_emb, src_feat_emb], dim=-1)\n",
    "        src_emb = self.concat_projection(src_emb_concat)\n",
    "        # Pass through Transformer\n",
    "        transformer_output = self.transformer(src_emb, tgt_emb, tgt_mask=self.tgt_mask)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.fc(transformer_output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device, time_stamp):\n",
    "    best_model = None\n",
    "    best_dev_loss = float('inf')\n",
    "    best_train_loss = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    loss_arr = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for seq1, feature1, seq2, _ in train_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            feature1 = torch.stack(feature1, dim=1)\n",
    "            feature1 = feature1.to(device).float()\n",
    "\n",
    "            # Shift target sequence for decoder input\n",
    "            tgt_input = seq2[:, :-1]\n",
    "            tgt_output = seq2[:, 1:]\n",
    "\n",
    "            outputs = model(seq1, tgt_input, feature1)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) #apply gradient clipping\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss /= len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "        loss_arr.append(total_loss)\n",
    "\n",
    "\n",
    "        # eval model\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            dev_loss = evaluate_model(model, dev_loader, criterion, device)\n",
    "            if dev_loss < best_dev_loss:\n",
    "                best_dev_loss = dev_loss\n",
    "                best_model = model\n",
    "                best_train_loss = total_loss\n",
    "                best_epoch = epoch\n",
    "    \n",
    "    # Save best model\n",
    "    if save_model:\n",
    "        torch.save(best_model.state_dict(), './model/'+time_stamp+'/transformer_model_best.pth')\n",
    "\n",
    "        import json\n",
    "        with open('./model/'+time_stamp+'/loss.json', 'w') as f:\n",
    "            json.dump({'loss': loss_arr, \n",
    "                    'best_epoch': best_epoch, \n",
    "                    'best_dev_loss': best_dev_loss, \n",
    "                    'best_train_loss': best_train_loss}, f, indent=4)\n",
    "            \n",
    "    return best_dev_loss\n",
    "\n",
    "def evaluate_model(model, dev_loader, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for seq1, feature1, seq2, _ in dev_loader:\n",
    "            seq1, seq2 = seq1.to(device), seq2.to(device)\n",
    "            feature1 = torch.stack(feature1, dim=1)\n",
    "            feature1 = feature1.to(device).float()\n",
    "\n",
    "            # Shift target sequence for decoder input\n",
    "            tgt_input = seq2[:, :-1]\n",
    "            tgt_output = seq2[:, 1:]\n",
    "\n",
    "            outputs = model(seq1, tgt_input, feature1)\n",
    "            loss = criterion(outputs.reshape(-1, outputs.size(-1)), tgt_output.reshape(-1).long())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Dev Loss: {total_loss / len(dev_loader):.4f}')\n",
    "    return total_loss / len(dev_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters sets\n",
    "hidden_dims = [64,128,256]\n",
    "num_layerss = [1,2,3,4]\n",
    "\n",
    "# keep track of the best dev loss and the corresponding hyperparameters\n",
    "loss_arrs = []\n",
    "best_dvl_hidden = []\n",
    "best_dvl_layer = []\n",
    "\n",
    "# Train model sequentially\n",
    "\n",
    "for i in range (len(hidden_dims)):\n",
    "    for j in range(len(num_layerss)):\n",
    "\n",
    "        print('Start trial ' + str(i+1))\n",
    "\n",
    "        input_dim = vocab_size\n",
    "        hidden_dim = hidden_dims[i]\n",
    "        feature_dim = 4\n",
    "        output_dim = vocab_size\n",
    "        num_layers = num_layerss[j]\n",
    "        nhead = 8\n",
    "        num_epochs = 60\n",
    "        learning_rate = 1e-3\n",
    "        batch_size = 32\n",
    "        time_stamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "        if save_model:\n",
    "            import os\n",
    "            if not os.path.exists('./model'):\n",
    "                os.makedirs('./model')\n",
    "            os.makedirs('./model/'+time_stamp)\n",
    "\n",
    "            import json\n",
    "            hyperparameters = {\n",
    "                'time_stamp': time_stamp,\n",
    "                'model': 'transformer',\n",
    "                'input_dim': input_dim,\n",
    "                'hidden_dim': hidden_dim,\n",
    "                'feature_dim': feature_dim,\n",
    "                'output_dim': output_dim,\n",
    "                'num_layers': num_layers,\n",
    "                'nhead': nhead,\n",
    "                'num_epochs': num_epochs,\n",
    "                'learning_rate': learning_rate,\n",
    "                'batch_size': batch_size\n",
    "            }\n",
    "            with open('./model/'+time_stamp+'/hyperparameters.json', 'w') as f:\n",
    "                json.dump(hyperparameters, f, indent=4)\n",
    "\n",
    "        # Device configuration\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "        print('Using ' + device)\n",
    "\n",
    "        # Load data\n",
    "        train_loader, dev_loader, test_loader = get_dataloaders(batch_size=batch_size, one_hot_encode=False, start_token=True, get_feature=True)\n",
    "\n",
    "        # Initialize model, criterion and optimizer\n",
    "        model = RNAPairTransformer(input_dim, hidden_dim, output_dim, feature_dim, num_layers, nhead, device).to(device)\n",
    "        weight = torch.tensor([1,1,1,1,2,0.01,1,1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.9 ** epoch)\n",
    "\n",
    "        # Train the model\n",
    "        loss_temp = train_model(model, train_loader, criterion, optimizer, num_epochs, device, time_stamp)\n",
    "        loss_arrs.append(loss_temp)\n",
    "        best_dvl_hidden.append(hidden_dim)\n",
    "        best_dvl_layer.append(num_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Result\n",
    "print(loss_arrs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
